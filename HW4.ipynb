{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49eaf30b",
   "metadata": {},
   "source": [
    "##### Introduction to Information Theory (Fall 2023/4)\n",
    "\n",
    "# Home Assignment 4\n",
    "\n",
    "#### Topics:\n",
    "- Lossless compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77440392",
   "metadata": {},
   "source": [
    "### 1. Operational Capacity of a Noiseless Channel\n",
    "1. Prove that for any channel $P_{X^n|Y^n}$ we have $C_{op} \\leq  \\log|\\mathcal X|$ without using the channel coding theorem (That is, directly from the definition.). Use the following steps:\n",
    " - Suppose that there exists a $(2^{nR},n)$-coding scheme with $R >\\log|\\mathcal X|$. Find the number of messages that cannot be uniquely mapped to a channel input.\n",
    " - Find the proportion of this number of messages out of the entire message space.\n",
    " - Argue that the average error in decoding is at least half this proportion.\n",
    " - Complete the proof.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c106c7",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "\n",
    "Since the assumption states that there exists a $(2^{nR},n)$-coding scheme with $R>log|\\mathcal{X}|$, this means that the scheme considered attempts to transmit information at a rate higher than the log of the size of the input alphabet. By definition of the $(2^{nR},n)$-coding scheme, the number of unique messages that can be sent over $n$ channel uses $2^{nR}$. However, the number of unique sentences that can be formed from the input alphabet over $n$ channel uses $|\\mathcal{X}|^n$.  \n",
    "Following that $R>log|\\mathcal{X}|$, then $2^{nR}>|\\mathcal{X}|^n$, which means that there are more unique messages that we wish to send than there are unique input sequences available: $$\\text{Number of messages that cannot uniquely be mapped to a channel input}=2^{nR}-|\\mathcal{X}|^n$$ Hence, the proportuon of this number of messages out of the entire message space can be calculated: $$\\frac{2^{nR}-|\\mathcal{X}|^n}{2^{nR}}=1-\\frac{|\\mathcal{X}|^n}{2^{nR}}$$\n",
    "Now, the non-uniquely mappable messages (whose proportion we just identified) cannot be reliably decoded because there's ambiguity: a signle received sequence could correspond to multiple possible sent messages. Therefore, the decoder might choose the wrong message among the possibilities and this contributes to decoding errors. Given that the errors are spread across the portion of the message space that exceeds $|\\mathcal{X}|^n$, it's reasonable to conclude that the decoding error for these messages will be significant, and we can define a conservative lower bound on this error rate as 50% of the proportion the the excess messages. 50% is a reasonable bound because if we consider two messages for every sequence that corresponds to multiple messages, the change of choosing the correct one is 0.5, implying a 50% error rate.  \n",
    "Therefore, if $R>log|\\mathcal{X}|$, there's a significant portion of the message space that cannot be uniquely mapped to the input sequences, which leads to an unavoidably high average decoding error. This however, contradicts the requirement for reliable communication, which need s that the average decoding error probability tends to zero as $n$ tends to infinity. Thus, for any channel $P_{X^n|Y^n}$, to ensure reliable communication (an average decoding error probability that approaches zero), the rate $R$ cannot exceed $log|\\mathcal{X}|$. Hence, $C_{op} \\leq  \\log|\\mathcal X|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fa12b",
   "metadata": {},
   "source": [
    "2. Consider a channel with identical input and output alphabets $\\mathcal X$. Assume that for some $m \\in \\mathbb N$ the channel is noiseless, i.e. \n",
    "$$\n",
    "P_{Y^m|X^m}(y^m|x^m) = \\begin{cases} 1 & x^m = y^m \\\\\n",
    "0 & x^m \\neq y^m.\n",
    "\\end{cases}\n",
    "$$\n",
    "Show that the operational capacity of this channel is $C_{op} := \\log |\\mathcal X|$ by proving an achievability and converse claims (you are not allowed to use the channel coding theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513e259",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa4c66",
   "metadata": {},
   "source": [
    "#### Achievability\n",
    "* Claim: It is possible to achieve a rate of $R=log|\\mathcal{X}|$ bits per channel use with arbitrarily low error probability for this noiseless channel.  $\\\\$\n",
    "* Proof: $\\\\$\n",
    "Given that we assume the channel is noiseless, if $x^m = y^m$, then the transmission has no errors. This means that one can encode a message directly into a sequence of symbols from the alphabet $\\mathcal{X}^m$ and expect the same sequence at the output.  \n",
    "Also, for a message of size $M$, we can represent $M=|\\mathcal{X}|^m$ unique messages since we can use $m$ channel uses to send any of $|\\mathcal{X}|^m$ the possible sequences. Hence, the rate in bits per channel use $R$ can be denoted as:\n",
    "$$R=\\frac{1}{m}logM=\\frac{1}{m}log|\\mathcal{X}|^m=log|\\mathcal{X}|$$\n",
    "For each use of the channel, $log|\\mathcal{X}|$ bits of information can be transmitted without error (since we're assuming no noise).  \n",
    "Now, since the channel is noiseless for sequences of length $m$, $P_{Y^m|X^m}(y^m|x^m) = 1$ if $x^m = y^m$ because the error probability is 0 for any message sent.\n",
    "\n",
    "#### Converse\n",
    "* Claim: It is not possible to reliably transmit at a rate higher than $log|\\mathcal{X}|$ bits per channel use. $\\\\$\n",
    "* Proof by contradiction: $\\\\$\n",
    "Assuming there exists a coding scheme that allows transmitting at a rate $R'>log|\\mathcal{X}|$ bits per channel use with arbitrarily low error probability.  \n",
    "That rate implies that the number of messages $M'$ that can be sent over $m$ uses of the channel exceeds $|\\mathcal{X}|^m$ since $M'=2^{mR'}>|\\mathcal{X}|^m$. But the noiseless channel can only distinguish $|\\mathcal{X}|^m$ unique input sequences over $m$ uses. Here is where the contradiction happens. There are more messages than unique sequences that can be transmitted which makes it impossible to uniquely map each message to a distinct sequence of channel inputs. However, given that we assumed a noiseless channel, every message must have a unique input sequence to be transmitted without error. Hence, exceeding $log|\\mathcal{X}|$ bits per channel use violates this requirement, making such a rate unachievable without introducing errors.  \n",
    "\n",
    "We can therefore conclude that a rate of $log|\\mathcal{X}|$ is attainable with zero error probability because of the noiseless nature of the channel over sequences of length $m$. Also, attempting to exceed the rate leads to a contradiction. Therefore we can conclude that the operational capacity of this channel is $log|\\mathcal{X}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5526f",
   "metadata": {},
   "source": [
    "### 2. Channel Capacity\n",
    "(Based on Exc. 7.6 in Thomas \\& Cover) Consider a 26-key typewriter.\n",
    "1. If pushing a key results in printing the associated letter, what is the capacity $C$ in bits?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664bf67",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdee9e7",
   "metadata": {},
   "source": [
    "If the type writer prints out what ever key is struck, then the output, Y, is the same as the input, X, and $C=maxI(X;Y)=maxH(X)=log26$, attained by a uniform distribution over the letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a9c8c",
   "metadata": {},
   "source": [
    "2. Now suppose that pushing a key results in printing the associated letter or the next letter in the alphabet with equal probability. That is, $A \\to B$, $B\\to C$,...,$Z \\to A$. What is the capacity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbbc51",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b3c24",
   "metadata": {},
   "source": [
    "In this case, the output is either equal to the input(with probability 0.5) or equal to the next letter(with probability 0.5). Hence $H(Y|X)=log2$ independent of the distribution of $X$, and hence $C=maxI(X;Y)=maxH(Y) - log2=log26 - log2 =log13$,  attained for a uniform distribution over the output, which in turn is attained by a uniform distribution on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6681fc2",
   "metadata": {},
   "source": [
    "3. What is the highest rate code with block length one $(n=1)$ that you can find that achieves *zero* probability of error for the channel in part (2)?\n",
    "\n",
    "*Hint for (2)*: \n",
    "Show first that for a channel with transition matrix in which the rows are permutations of each other and the columns are permutations of each other, the capacity is \n",
    "$$\n",
    "C = \\log |\\mathcal Y| - H( \\text{row of transition matrix}).\n",
    "$$\n",
    "(such a channel is called *symmetric*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413eaad",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd127abf",
   "metadata": {},
   "source": [
    "A simple zero error block lengthone code is the one that uses every alternate letter, say $A,C,E,...,W,Y$. In this case, none of the code words will be confused, since $A$ will produce either $A$ or $B$, $C$ will produce $C$ or $D$, etc. The rate of this code:\n",
    "\n",
    "$$\n",
    "R= \\frac{log(\\#codewords)}  {Blocklength} = \\frac{log13}{1} = log13\n",
    "$$\n",
    "\n",
    "In this case, we can achieve capacity with a simple code with zero error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3465b",
   "metadata": {},
   "source": [
    "### Channel Capacity and Random Codes\n",
    "(based on Exc. 7.8 and 7.9 in Thomas \\& Cover)\n",
    "The $Z$-channel has a binary input and output alphabets and transition probabilities $P_{Y|X}$ given by the matrix\n",
    "$$\n",
    "P_{Y|X} = \\begin{bmatrix} 1 & 0 \\\\\n",
    "1/2 & 1/2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Namely, $\\Pr[Y=0|X=0]=1$ while $\\Pr[Y=0|X=1]=1/2$ ($0$ goes noiselessly, while a $1$ may turn into a zero with probability $1/2$). \n",
    "1. Find the capacity of this channel and the maximizing input probability distribution. It may help to know that \n",
    "$$\n",
    "\\frac{d}{dp} h_2(p) = \\log_2((1-p)/p). \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd589d",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42d3d1",
   "metadata": {},
   "source": [
    "First we express $I(X;Y)$, the mutual information between the input and output of the $Z-channel$, as a function of:\n",
    "$$\n",
    "x=P(X=1):\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(Y|X) = P(X=0)·0+P(X=1)·1 = x \n",
    "$$\n",
    "\n",
    "$$\n",
    "H(Y) = H(P(Y=1))=H(x/2) \n",
    "$$\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(Y)H(Y|X)=H(x/2) - x \n",
    "$$\n",
    "\n",
    "Since $I(X;Y)=0$ when $x=0$ and $x=1$, the maximum mutual information is obtained for some value of $x$ such that $0<x<1$. Using elementary calculus,we determine that\n",
    "$$\n",
    "\\frac{d} {dx}I(X;Y)=0.5log_2\\frac{1 - \\frac{x}{2}} {\\frac{x}{2}} - 1, \n",
    "$$\n",
    "\n",
    "\n",
    "which is equal to zero for $x=\\frac{2}{5}$. (It is reasonable that $P(X=1)< 0.5$ because $X=1$ is the noisy input to the channel) So the capacity of the $Z-channel$ in bits is $H(\\frac{1}{5}) - \\frac{2}{5}=0.722 - 0.4 = 0.322$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9d878",
   "metadata": {},
   "source": [
    "2. Assume that we draw a random $(2^{nR},n)$ code (as in the proof of the channel coding theorem) for this channel in which each codeword is a sequence of *fair* coin tosses (this may not achieve capacity). Find the maximum rate $R$ such that the probability of error $P_{\\mathrm{err}}^{(n)}$ averaged over the randomly generated codes, tends to zero as $n$ tends to infinity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26f24a",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479a1c1",
   "metadata": {},
   "source": [
    "From the proof of the channel coding theorem, it follows that using a random code with code words generated according to probability $p(x)$, we can send information at a rate $I(X;Y)$ corresponding to that $p(x)$ with an arbitrarily low probability of error. For the $Z-channel$ described in the previous question, we can calculate $I(X;Y)$ for a uniform distribution on the input. The distribution on $Y$ is $(\\frac{3}{4}, \\frac{1}{4})$, and therefore:\n",
    "$$\n",
    "I(X;Y)=H(Y) - H(Y|X) = H(\\frac{3}{4}, \\frac{1}{4}) - 0.5H(\\frac{1}{2}, \\frac{1}{2})=\\frac{3}{2} -  \\frac{3}{4}log3\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
