{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49eaf30b",
   "metadata": {},
   "source": [
    "##### Introduction to Information Theory (Fall 2023/4)\n",
    "\n",
    "# Home Assignment 2\n",
    "\n",
    "#### Topics:\n",
    "- Lossless compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3d406",
   "metadata": {},
   "source": [
    "### 1. Mismatch in Variable Length Coding and Distribution Divergence\n",
    "(there are two items in this question)\n",
    "\n",
    "In the class we proved the following results on the expected length $L$ of any prefix free code applied to a set of symbols sampled independently from a distribution $P_X$.\n",
    "$$\n",
    "H(X) \\leq \\ex{L} \\leq H(X)+1.\n",
    "$$\n",
    "Recall that the code attaining expected length $\\leq H(X)+1$ assigns $\\lceil -\\log P_X(x) \\rceil$ bits to the symbol $x \\in \\Xcal$. \n",
    "Suppose now we design this code to symbols from a distribution $Q_X \\neq P_X$. \n",
    "- Show that the expected length is bounded from above by\n",
    "$$\n",
    "H(P_X|| Q_X) := -\\mathbb E_{X \\sim P_X}[{\\log[Q_X(X)]}] = -\\sum_{x \\in \\Xcal} P_X(x) \\log Q_X(x) =  -\\sum_{x \\in \\Xcal} p(x) \\log q(x)\n",
    "$$\n",
    "By doing so, you will prove that for any PMF $Q$ on $\\Xcal$ there exists a prefix-free code $f$ such that for any PMF $P$ on $\\Xcal$ the average code length for encoding symbols satisfies\n",
    "$$\n",
    "\\mathbb E_{X \\sim P}[{len(f(X))}] \\leq H(P||Q) + 1\n",
    "$$\n",
    "\n",
    "*Note:* $H(P||Q)$ is called the **cross entropy** of $Q$ under $P$, and we have $H(P||Q) = H(P) + D(P||Q)$ where \n",
    "$$\n",
    "D(P||Q) := -\\sum_{x\\in \\Xcal} p(x) \\log \\frac{q(x)}{p(x)},\n",
    "$$\n",
    "is the **relative entropy** or the **Kullback-Leibler divergence** between $P$ to $Q$.\n",
    "\n",
    "- Prove that $D(P||Q) \\geq 0$, hence mismatch necessarily increases the expected length. *Hint:* Use that $\\ln(x) \\leq x - 1$, for $x\\geq 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9901cb6",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e89ce",
   "metadata": {},
   "source": [
    "\n",
    "#### 1)\n",
    "The expected length is bounded by the Kullback-Leibler divergence between\n",
    "$P_X$ and $Q_X$.\n",
    "\n",
    "Now\n",
    "$$\n",
    "H(P||Q) = -\\sum_{x\\in \\Xcal} P_X(x)\\log Q_X(x)\n",
    "$$\n",
    "\n",
    "Using Kullback-Leibler divergence:\n",
    "\n",
    "$$\n",
    "E(L) = -\\sum_{x\\in \\Xcal} P_X(x) \\log (\\frac{Q_X(x)}{P_X(x)})\n",
    "$$\n",
    "$$\n",
    "E(L) = -\\sum_{x\\in \\Xcal} P_X(x) \\log Q_X(x) + \\sum_{x\\in \\Xcal} P_X(x) \\log P_X(x)\n",
    "$$\n",
    "\n",
    "Using the entropy $H(P_X)$:\n",
    "$$\n",
    "-\\sum_{x\\in \\Xcal} P_X(x) \\log Q_X(x) + H(P_X)\n",
    "$$\n",
    "\n",
    "And $H(P_X) \\leq  \\log(x)$ because of jensen inequality\n",
    "\n",
    "$$\n",
    "-\\sum_{x\\in \\Xcal} P_X(x) \\log Q_X(x) + \\log(x)\n",
    "$$\n",
    "\n",
    "### 2)\n",
    "\n",
    "Now using:\n",
    "$$\n",
    "D(P||Q) = -\\sum_{x\\in \\Xcal} p(x) \\log \\frac{q(x)}{p(x)},\n",
    "$$\n",
    "Using the hint:\n",
    "\n",
    "$$\n",
    "- \\log (\\frac{q(x)}{p(x)}) = \\log (\\frac{p(x)}{q(x)}) \\leq \\frac{p(x)}{q(x)} - 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "D(P||Q) \\geq - \\sum_{x\\in \\Xcal} P(x)\\frac{p(x)}{q(x)} + \\sum_{x\\in \\Xcal} P(x)\n",
    "$$\n",
    "Now:\n",
    "\n",
    "\n",
    "$$\n",
    "E_P(\\frac{p(x)}{q(x)}) = P(x)\\frac{p(x)}{q(x)} \\geq 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "D(P||Q) \\geq -1 + \\sum_{x\\in \\Xcal} p(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "D(P||Q) \\geq -1 + 1\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\n",
    "D(P||Q) \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030bc73",
   "metadata": {},
   "source": [
    "### 2. Huffman Coding and Alphabet Tensorization\n",
    "\n",
    "In this exercise you will compress a random binary string sampled from a binary source using a Huffman code. Every symbol consists of $k$ consecutive binary digits stacked together.\n",
    "1. Complete the code prototype for compression based on the Huffman code: \n",
    "    - Write a function that receives as an input a set of symbols and their probabilities and returns the Huffman code of each symbol. \n",
    "    - Write a function that compresses (encodes) a list of binary words (symbols) using the code. \n",
    "    - Write a function that decompress the encoded string given the compressed string and the code. \n",
    "\n",
    "2. Generate a Huffman code for length-$k$ binary words sampled from $ Bernulli^k(p)$ ($k$ independent samples from $ Bernulli(p)$). Use $p=0.2$.\n",
    "    - What is the expected code length when $k=2,3,4,5,6,7,8$?\n",
    "    - What is the expected code length as $k \\to \\infty$ (use the analytic expression)?\n",
    "\n",
    "3. Sample a binary string from $ Bernulli(p)$ of length $n = 2^{10}$ using the provided function (do not change the seed). Encode the string using the code you created in (2) for $k=2,3,4,5,6,7,8$. What is the actual length of the code for every $k$? Compare this to the expected code length in (2) and to the expected length as $k \\to \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca654590",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def sample_n_times_from_Bernuolli(n: int, p: float, seed = SEED) -> list:\n",
    "    \"\"\"\n",
    "    Sample from Bernoulli distribution with parameter p\n",
    "    \"\"\"\n",
    "    assert 0 <= p <= 1\n",
    "    assert n > 0\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    return [int(random.random() < p) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6ee40",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "#### 1) \n",
    "The steps to build a Huffman Tree are as follows:\n",
    "1. Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue. The value of probability is used to compare two nodes in min heap. Initially, the least probable character is at root)\n",
    "2. Extract two nodes with the minimum probability from the min heap.\n",
    "3. Create a new internal node with a probability equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap.\n",
    "4. Repeat steps 2 and 3 until the heap contains only one node. The remaining node is the root node and the tree is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b39357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, symbol, prob):\n",
    "        self.symbol = symbol\n",
    "        self.prob = prob\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.prob < other.prob\n",
    "\n",
    "def huffman_code(symbols_and_probabilities: dict) -> dict:\n",
    "    \"\"\"\n",
    "    >>> huffman_code({'A': 0.5, 'B': 0.3, 'C': 0.2})\n",
    "    {'A': '0', 'B': '10', 'C': '11'}\n",
    "\n",
    "    Args:\n",
    "        symbols_and_probabilities: dict with symbols and their probabilities\n",
    "    \n",
    "    Returns:\n",
    "        dict with symbols and their Huffman codes\n",
    "    \"\"\"\n",
    "\n",
    "    # Build Huffman tree\n",
    "    def build_tree(symbols_and_probabilities):\n",
    "        heap = [Node(s, p) for s, p in symbols_and_probabilities.items()]\n",
    "        heapq.heapify(heap)\n",
    "        while len(heap) > 1:\n",
    "            left = heapq.heappop(heap)\n",
    "            right = heapq.heappop(heap)\n",
    "            node = Node(None, left.prob + right.prob)\n",
    "            node.left = left\n",
    "            node.right = right\n",
    "            heapq.heappush(heap, node)\n",
    "        return heap[0]\n",
    "    \n",
    "    # Traverse Huffman tree\n",
    "    def traverse_tree(node, prefix, code):\n",
    "        if node.symbol:\n",
    "            code[node.symbol] = prefix\n",
    "        else:\n",
    "            traverse_tree(node.left, prefix + '0', code)\n",
    "            traverse_tree(node.right, prefix + '1', code)\n",
    "\n",
    "    # Create Huffman code\n",
    "    root = build_tree(symbols_and_probabilities)\n",
    "    code = {}\n",
    "    traverse_tree(root, '', code)\n",
    "    return code\n",
    "\n",
    "\n",
    "def encode_Huffman(data: list, code: dict) -> list:\n",
    "    \"\"\"\n",
    "    >>> encode_Huffman([0, 1, 0, 0, 1, 1, 0], {0: '0', 1: '1'})\n",
    "    '0100110'\n",
    "\n",
    "    Args:\n",
    "        data: list of symbols\n",
    "        code: dict with symbols and their Huffman codes\n",
    "    \n",
    "    Returns:\n",
    "        string with encoded data\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode a list of symbols\n",
    "    return ''.join(code[symbol] for symbol in data)\n",
    "\n",
    "def decode_Huffman(encoded_data: list, code: dict) -> list:\n",
    "    \"\"\"\n",
    "    >>> decode_Huffman('0100110', {0: '0', 1: '1'})\n",
    "    [0, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "    Args:\n",
    "        encoded_data: string with encoded data\n",
    "        code: dict with symbols and their Huffman codes\n",
    "    \n",
    "    Returns:\n",
    "        list of symbols\n",
    "    \"\"\"\n",
    "\n",
    "    # Reverse Huffman code\n",
    "    reversed_code = {v: k for k, v in code.items()}\n",
    "    decoded_data = []\n",
    "    symbol = ''\n",
    "\n",
    "    # Decode an encoded string back to a list of symbols\n",
    "    for bit in encoded_data:\n",
    "        symbol += bit\n",
    "        if symbol in reversed_code:\n",
    "            decoded_data.append(reversed_code[symbol])\n",
    "            symbol = ''\n",
    "    return decoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25880459",
   "metadata": {},
   "source": [
    "We proceed to implement the function in an example to check that the functions are performing as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c313c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huffman Codes: {'C': '00', 'D': '010', 'A': '011', 'B': '10', 'E': '11'}\n",
      "Encoded Message: 1110110001010011\n",
      "Decoded Message: ['E', 'B', 'E', 'C', 'D', 'B', 'A']\n",
      "Decoding Successful: True\n"
     ]
    }
   ],
   "source": [
    "# Define a set of symbols with their probabilities\n",
    "symbols_and_probabilities = {'A': 0.1, 'B': 0.3, 'C': 0.2, 'D': 0.1, 'E': 0.3}\n",
    "\n",
    "# Generate Huffman codes for the symbols\n",
    "huffman_codes = huffman_code(symbols_and_probabilities)\n",
    "print(\"Huffman Codes:\", huffman_codes)\n",
    "\n",
    "# Define a message using the symbols\n",
    "message = ['E', 'B', 'E', 'C', 'D', 'B', 'A']\n",
    "\n",
    "# Encode the message using the Huffman codes\n",
    "encoded_message = encode_Huffman(message, huffman_codes)\n",
    "print(\"Encoded Message:\", encoded_message)\n",
    "\n",
    "# Decode the encoded message back to symbols\n",
    "decoded_message = decode_Huffman(encoded_message, huffman_codes)\n",
    "print(\"Decoded Message:\", decoded_message)\n",
    "\n",
    "# Check if the decoded message matches the original message\n",
    "print(\"Decoding Successful:\", decoded_message == message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3596dd",
   "metadata": {},
   "source": [
    "#### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac43af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For k=2 the expected code length is 0.7800000000000002 bits\n",
      " For k=3 the expected code length is 0.7280000000000002 bits\n",
      " For k=4 the expected code length is 0.7408000000000002 bits\n",
      " For k=5 the expected code length is 0.7379199999999998 bits\n",
      " For k=6 the expected code length is 0.7252480000000001 bits\n",
      " For k=7 the expected code length is 0.7317558857142865 bits\n",
      " For k=8 the expected code length is 0.7322281599999974 bits\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "expected_lengths = {}\n",
    "\n",
    "# Calculate the probability of a word independently sampled from a Bernoulli distribution with probability p\n",
    "def calculate_probability(word, p):\n",
    "    return p**sum(word) * (1-p)**(len(word) - sum(word))\n",
    "\n",
    "# Calculate the expected code length\n",
    "def expected_code_length(code, probabilities):\n",
    "    return sum(len(code[word]) * prob for word, prob in probabilities.items())\n",
    "\n",
    "# Define the probability of a 1, probability of 0 is 1 - p\n",
    "p = 0.2\n",
    "\n",
    "# Calculate the expected code length for all possible length-k binary words\n",
    "for k in range(2, 9):\n",
    "    # 1. Generate all possible length-k binary words\n",
    "    words = list(itertools.product([0, 1], repeat=k))\n",
    "\n",
    "    # Calculate the probability of each word\n",
    "    probabilities = {''.join(map(str, word)): calculate_probability(word, p) for word in words}\n",
    "\n",
    "    # Generate Huffman codes\n",
    "    huffman_codes = huffman_code(probabilities)\n",
    "\n",
    "    # Calculate expected code length\n",
    "    expected_length = expected_code_length(huffman_codes, probabilities)/k\n",
    "\n",
    "    # Print the expected code length for length-k binary words with k in [2, 8]\n",
    "    print(f\" For k={k} the expected code length is {expected_length} bits\")\n",
    "\n",
    "    # Store the expected lengths of the encoded string\n",
    "    expected_lengths[k] = expected_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5a21b",
   "metadata": {},
   "source": [
    "As seen in the lectures, Shannon's source coding theorem for discrete memoryless sources (like Bernouilli process) states: $$H(x) \\leq \\bar{L} \\leq H(x)+1$$\n",
    "Which implies that the entropy represents the fundamental limit of compresibility given the probabilities of the symbols. The upper bound is meaningless in this case since we are dealing with a binary source (Bernouilli). Therefore, as the sequence length $k$ increases, the average code length per symbol gets closer to the entropy. Hence,\n",
    "$$H(p)=-plog(p)-(1-p)log(1-p)$$\n",
    "$$H(0.2)=-0.2log(0.2)-(0.8)log(0.8)=0.7219 bits$$\n",
    "Finally, as $k$ tends to infinity, the expected code length for $k$-length binary words smapled from a $Bernouilli^k(0.2)$ distribution will be close to 0.7219."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177bee5",
   "metadata": {},
   "source": [
    "#### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffa244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For k=2 the actual code length is 0.755859375 bits\n",
      " For k=3 the actual code length is 0.6865234375 bits\n",
      " For k=4 the actual code length is 0.6943359375 bits\n",
      " For k=5 the actual code length is 0.6875 bits\n",
      " For k=6 the actual code length is 0.68359375 bits\n",
      " For k=7 the actual code length is 0.6943359375 bits\n",
      " For k=8 the actual code length is 0.6953125 bits\n"
     ]
    }
   ],
   "source": [
    "# Generate a binary string of length 2^10\n",
    "binary_string = sample_n_times_from_Bernuolli(2**10, 0.2, seed = SEED)\n",
    "\n",
    "# Encode and compare lengths for k = 2, 3, 4, 5, 6, 7, 8\n",
    "actual_lengths = {}\n",
    "\n",
    "for k in range(2, 9):\n",
    "\n",
    "    # Generate all possible length-k binary words\n",
    "    words = list(itertools.product([0, 1], repeat=k))\n",
    "\n",
    "    # Calculate the probability of each word\n",
    "    probabilities = {''.join(map(str, word)): calculate_probability(word, p) for word in words}\n",
    "\n",
    "    # Generate Huffman codes\n",
    "    huffman_codes = huffman_code(probabilities)\n",
    "\n",
    "    encoded_string = ''\n",
    "\n",
    "    # Encode the sampled string in chunks of size k\n",
    "    for i in range(0, len(binary_string), k):\n",
    "        chunk = binary_string[i:i+k]\n",
    "\n",
    "        # If the last chunk is smaller than k, pad it with 0s\n",
    "        if len(chunk) == k:\n",
    "            # Encode the chunk using the Huffman codes\n",
    "            encoded_string += huffman_codes[''.join(map(str, chunk))]\n",
    "\n",
    "\n",
    "    # Print the expected code length for length-k binary words with k in [2, 8]\n",
    "    # Divide the length of the encoded string by 1024 to normalize it based on the actual length of the original binary string 2^10=1024\n",
    "    print(f\" For k={k} the actual code length is {len(encoded_string)/1024} bits\")\n",
    "    # Store the actual length of the encoded string\n",
    "    actual_lengths[k] = len(encoded_string)/1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07bceddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=2\n",
      "The expected length of the code is: 0.7800000000000002\n",
      "The actual length of the code is: 0.755859375\n",
      "The difference between the expected and actual length is: 0.02414062500000025\n",
      "For k=3\n",
      "The expected length of the code is: 0.7280000000000002\n",
      "The actual length of the code is: 0.6865234375\n",
      "The difference between the expected and actual length is: 0.0414765625000002\n",
      "For k=4\n",
      "The expected length of the code is: 0.7408000000000002\n",
      "The actual length of the code is: 0.6943359375\n",
      "The difference between the expected and actual length is: 0.046464062500000236\n",
      "For k=5\n",
      "The expected length of the code is: 0.7379199999999998\n",
      "The actual length of the code is: 0.6875\n",
      "The difference between the expected and actual length is: 0.0504199999999998\n",
      "For k=6\n",
      "The expected length of the code is: 0.7252480000000001\n",
      "The actual length of the code is: 0.68359375\n",
      "The difference between the expected and actual length is: 0.041654250000000115\n",
      "For k=7\n",
      "The expected length of the code is: 0.7317558857142865\n",
      "The actual length of the code is: 0.6943359375\n",
      "The difference between the expected and actual length is: 0.037419948214286514\n",
      "For k=8\n",
      "The expected length of the code is: 0.7322281599999974\n",
      "The actual length of the code is: 0.6953125\n",
      "The difference between the expected and actual length is: 0.03691565999999735\n"
     ]
    }
   ],
   "source": [
    "n = 1024\n",
    "for k in range(2,9):\n",
    "    print(f\"For k={k}\")\n",
    "    print(f\"The expected length of the code is: {expected_lengths[k]}\")\n",
    "    print(f\"The actual length of the code is: {actual_lengths[k]}\")\n",
    "    print(f\"The difference between the expected and actual length is: {expected_lengths[k] - actual_lengths[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea1df8",
   "metadata": {},
   "source": [
    "We observe that the differences between the expected and actual code length are consideratively small. Also, we can observe that for both cases the code length gets closer to the theoretical value, which is the entropy of 0.7219. Indeed, Huffman coding is more efficient when it has more symbols to work with (that allows it to better approximate the probabilities of various symbol sequences). It is important to note that Huffman coding may not always achieve the theoretical entropy, especially for finite chunk sizes and non-ideal probability distributions. The values obtained for the sampled binary string range between 0.6836 and 0.7559 bits, so they are around the theoretical entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e5a5f",
   "metadata": {},
   "source": [
    "### 3. Entropy and Stochastic Process\n",
    "(there are 5 items in this question)\n",
    "Let $X \\sim Bernulli(p)$ and $Y \\sim Bernulli(q)$, $p \\neq q$. Let\n",
    "$$\n",
    "Z := X \\oplus Y ?\n",
    "$$\n",
    "- What is the distributio\n",
    "n of $Z$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ddbff",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "The distribution of $Z$ is $Z\\sim Bernulli(p + q - 2pq)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a114ec",
   "metadata": {},
   "source": [
    "- What is the entropy of $Z$? is it smaller or larger than $\\max\\{H(X), H(Y)\\}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70a843",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "Let\n",
    "$$\n",
    "H(P) = -[p\\log_2(p) + (1 - p)\\log_2(1-p)] \n",
    "$$ \n",
    "Now using $p + q -2pq$ for $p$:\n",
    "$$\n",
    "H(Z) = - [(p + q - 2pq)\\log_2(p + q - 2pq) + (1 - p - q + 2pq)\\log_2(1 - p - q + 2pq)]\n",
    "$$\n",
    "$$\n",
    "H(X) = -[p\\log_2(p) + (1 - p)\\log_2(1 - p)]\n",
    "$$\n",
    "$$\n",
    "H(Y) = -[q\\log_2(q) + (1 - q)\\log_2(1 - q)]\n",
    "$$\n",
    "\n",
    "\n",
    "Now, comparing $H(Z)$ with $\\max\\{H(X), H(Y)\\}$, we observe that $H(Z)$ is larger than both $H(X)$ and $H(Y)$. This is because the XOR operation introduces additional uncertainty or randomness compared to individual Bernoulli variables $X$ and $Y$. Therefore, the entropy of $$ is greater than the maximum entropy among $X$ and $Y$, indicating that $Z$ is a more unpredictable random variable than either $X$ or $Y$ alone and therefore, $H(Z)$ is bigger than $H(X)$ and $H(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fad9e6",
   "metadata": {},
   "source": [
    "Let $W^n \\simiid Bernulli(p)$. Consider the following stochastic process:\n",
    "$$\n",
    "X_1 \\sim Bernulli(q),\n",
    "$$\n",
    "$$\n",
    "X_{n+1} = X_n \\oplus W_{n+1}, \\quad n \\geq 1.\n",
    "$$\n",
    "($\\oplus$ is addition modulo 2 or the XOR operation)\n",
    "- Is $X^n$ a Markov chain? If yes, find its stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5669d2",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "The conditional probability $P(X_{n+1} = 1 | X_n = x) $ for $x$ = 1, 0\n",
    "\n",
    "Given that $X_{n+1} = X_n \\oplus W_{n+1}$:\n",
    "$$\n",
    "P(X_{n+1} = 1 | X_n = 0) = P(W_{n+1} = 1) = p\n",
    "$$\n",
    "$$\n",
    "P(X_{n+1} = 0 | X_n = 1) = P(W_{n+1} = 1) = 1 - p\n",
    "$$\n",
    "\n",
    "The transition matrix is $P = \\begin{bmatrix} 1 - p & p \\\\\n",
    "\\ 1 - p & p\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$X_n$ is a markov chain. so let $[\\pi_0, \\pi_1] $ be the stationary distribution.\n",
    "$$\n",
    "\\pi_0(1-p) +\\pi_1(1-p) = \\pi_0 + \\pi_0p + \\pi_1p = \\pi_1\n",
    "$$\n",
    "\n",
    "Now $\\pi_0p + \\pi_1p = \\pi_1$, $\\pi_0p = 0$. since $p \\neq 0$ and $\\pi_0 \\neq 0$ so $\\pi_1(1-p) = 0$\n",
    "\n",
    "Now if $\\pi_0 \\neq 1$ then $\\pi_1 \\neq 0$. however, of $\\pi_1 = 1$ so the stationary distribution is $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344840ec",
   "metadata": {},
   "source": [
    "- If $X^n$ is a Markov chain, is it also ergodic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f7184",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "A Markov chain is irreducible $P_01 = P_10$ = 1. A Markov chain is periodic if the gcd of the set of all possible return times to any state is 1. For $X_n$ the return times to each state is 2. The gcd = 2 which is not equal to 1. So $X_n$ is not a\n",
    "periodic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acaeb7",
   "metadata": {},
   "source": [
    "- Is $X^n$ a stationary process? (your answer may depend on $q$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4f1e0",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "A stationary of $X_n$ as a process depend on the specfic value of $q$. If $q=0.5$, it may exhibit stationary but $q$ different from 0.5 so the process is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b64359",
   "metadata": {},
   "source": [
    "### 4. Compressing a Markov Process\n",
    "(there are 5 items in this question)\n",
    "In this question you will sample a sequence from a two-states Markov source and compress this sequence in a losslessly manner using several methods. The function ``sample_Markov_path`` below samples such a sequence. \n",
    "\n",
    "Use the transition matrix \n",
    "$$\n",
    "Q = \\begin{bmatrix} 1-\\alpha & \\alpha \\\\\n",
    "\\beta & 1- \\beta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and vector of initial probabilities $\\begin{bmatrix} 1, 0 \\end{bmatrix}$ (namely, you begin at state $0$). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b94a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benka\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def sample_Markov_path(Q: np.ndarray, initial_probs: np.ndarray, n: int)->np.ndarray:\n",
    "    \"\"\"\n",
    "    Sample from a path from a Markov chain\n",
    "    \n",
    "    Args:\n",
    "        :Q:  transition probability matrix\n",
    "        :initial_probs:  vector of probabilities of the initial state\n",
    "        :n:  length of sample path\n",
    "    \n",
    "    Return:\n",
    "        :xx:  sample from the Markov chain of length n\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    M = Q.shape[0]\n",
    "    xx = np.zeros((n,M))\n",
    "\n",
    "    prob_vec = initial_probs\n",
    "\n",
    "    for i in range(n):\n",
    "        xx[i] = multinomial.rvs(p=prob_vec, n=1, random_state=SEED+i)\n",
    "        prob_vec = xx[i] @ Q\n",
    "\n",
    "    return np.argmax(xx, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3a74e",
   "metadata": {},
   "source": [
    "A short sample from the Markov chain (set $n = 2^{14}$ when solving the assignment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d31dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.15\n",
    "\n",
    "Q = np.array([\n",
    "    [1-alpha, alpha],\n",
    "    [beta, 1-beta] \n",
    "])\n",
    "\n",
    "initial_probs = [1, 0]  # start at state 0\n",
    "X = sample_Markov_path(Q, initial_probs, n = 100)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cec069",
   "metadata": {},
   "source": [
    "**(1) What is the entropy rate of this process? is it smaller or larger than the entropy of the stationary distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b042a2",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e528e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stationary distribution of the Markov chain is [0.6 0.4]\n",
      "The entropy rate of the Markov chain is 0.5253334780401289 bits\n",
      "The entropy of the stationary distribution is 0.9709505944546688 bits\n"
     ]
    }
   ],
   "source": [
    "# Solving pi * Q = pi, with the condition that sum of pi = 1\n",
    "pi = np.linalg.solve(np.transpose(Q) - np.eye(2) + np.ones((2,2)), np.ones(2))\n",
    "pi /= np.sum(pi)\n",
    "\n",
    "# Compute the entropy rate of the Markov process\n",
    "entropy_rate = -np.sum(pi[:, np.newaxis] * Q * np.log2(Q))\n",
    "\n",
    "# Compute the entropy of the stationary distribution\n",
    "entropy_stationary = -np.sum(pi * np.log2(pi))\n",
    "\n",
    "print(f\"The stationary distribution of the Markov chain is {pi}\")\n",
    "print(f\"The entropy rate of the Markov chain is {entropy_rate} bits\")\n",
    "print(f\"The entropy of the stationary distribution is {entropy_stationary} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc249f",
   "metadata": {},
   "source": [
    "The entropy rate is smaller than the entropy of the stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fcbe02",
   "metadata": {},
   "source": [
    "**(2) With $\\alpha=.1$ and $\\beta=.15$, generate a binary string of length $n=2^{14}$ from this Markov chain (using the function ``sample_Markov_path``). What is the fraction of times you spent at each state? Verify that this fraction matched more or less the stationary distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d72371",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2519c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of time spent in each state is [0.58355713 0.41644287]\n"
     ]
    }
   ],
   "source": [
    "# Length of the binary string\n",
    "n = 2**14\n",
    "\n",
    "# Generate the binary string from the Markov chain\n",
    "X = sample_Markov_path(Q, initial_probs, n)\n",
    "\n",
    "# Calculate the fraction of time spent in each state\n",
    "fraction_per_state = np.array([np.sum(X == state) / n for state in range(2)])\n",
    "print(f\"The fraction of time spent in each state is {fraction_per_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe0731",
   "metadata": {},
   "source": [
    "For a binary string of length $2^{14}$ the fraction of time spent in each state is $[0.5835 , 0.4164]$. This result is very close to the stationary distribution calculated in (1) was $\\pi = [0.6 , 0.4]$. Hence, over a large number of steps, the Markov chain indeed spends roughly 60% of the time in state 0 and 40% of the time in state 1 as predicted by the stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6baf64",
   "metadata": {},
   "source": [
    "**(3) Compress the binary string using a Huffman code for tuples of 8 symbols (one byte), i.e., consider the tensorized source with $K=8$. Estimate tuple frequencies either from the data (easier) or directly from the model. Plot the frequencies of the $2^K$ tuples. Can you anticipate the compression rate (``bits_compressed`` / ``bits_original``) without actually do the encoding?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660052c",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00142cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of each chunk is [0.27050781 0.03710938 0.00390625 0.02832031 0.00488281 0.00048828\n",
      " 0.00585938 0.02490234 0.00292969 0.00439453 0.00048828 0.00683594\n",
      " 0.02978516 0.00683594 0.00048828 0.00634766 0.00048828 0.00146484\n",
      " 0.00390625 0.00439453 0.03027344 0.00390625 0.00048828 0.00048828\n",
      " 0.00390625 0.00048828 0.00048828 0.00488281 0.00048828 0.00097656\n",
      " 0.00439453 0.00048828 0.00439453 0.02587891 0.00585938 0.00048828\n",
      " 0.00048828 0.00048828 0.00048828 0.00097656 0.00634766 0.00048828\n",
      " 0.00048828 0.00048828 0.00097656 0.00488281 0.00097656 0.00048828\n",
      " 0.00048828 0.00488281 0.00048828 0.00097656 0.00244141 0.00341797\n",
      " 0.0234375  0.02734375 0.00195312 0.00195312 0.00390625 0.00048828\n",
      " 0.00244141 0.00097656 0.00048828 0.00341797 0.00195312 0.00292969\n",
      " 0.00146484 0.00097656 0.00048828 0.00048828 0.00048828 0.00048828\n",
      " 0.00048828 0.00146484 0.02734375 0.00390625 0.00048828 0.00537109\n",
      " 0.00048828 0.00244141 0.00097656 0.00097656 0.00048828 0.00292969\n",
      " 0.00097656 0.00048828 0.00195312 0.02880859 0.00292969 0.00585938\n",
      " 0.00097656 0.00048828 0.00341797 0.00146484 0.00048828 0.00097656\n",
      " 0.00488281 0.03222656 0.00292969 0.00195312 0.00488281 0.00048828\n",
      " 0.00048828 0.00195312 0.02685547 0.00244141 0.00097656 0.00390625\n",
      " 0.02050781 0.00341797 0.0234375  0.13427734]\n",
      "The probability of each chunk is {'00000000': 0.2705078125, '00000001': 0.037109375, '00000010': 0.00390625, '00000011': 0.0283203125, '00000100': 0.0048828125, '00000101': 0.00048828125, '00000110': 0.005859375, '00000111': 0.02490234375, '00001000': 0.0029296875, '00001100': 0.00439453125, '00001101': 0.00048828125, '00001110': 0.0068359375, '00001111': 0.02978515625, '00010000': 0.0068359375, '00010111': 0.00048828125, '00011000': 0.00634765625, '00011001': 0.00048828125, '00011011': 0.00146484375, '00011100': 0.00390625, '00011110': 0.00439453125, '00011111': 0.0302734375, '00100000': 0.00390625, '00100011': 0.00048828125, '00101111': 0.00048828125, '00110000': 0.00390625, '00110110': 0.00048828125, '00110111': 0.00048828125, '00111000': 0.0048828125, '00111001': 0.00048828125, '00111011': 0.0009765625, '00111100': 0.00439453125, '00111101': 0.00048828125, '00111110': 0.00439453125, '00111111': 0.02587890625, '01000000': 0.005859375, '01000001': 0.00048828125, '01000111': 0.00048828125, '01001111': 0.00048828125, '01011000': 0.00048828125, '01011111': 0.0009765625, '01100000': 0.00634765625, '01100001': 0.00048828125, '01100011': 0.00048828125, '01100101': 0.00048828125, '01101111': 0.0009765625, '01110000': 0.0048828125, '01110001': 0.0009765625, '01110011': 0.00048828125, '01110111': 0.00048828125, '01111000': 0.0048828125, '01111001': 0.00048828125, '01111011': 0.0009765625, '01111100': 0.00244140625, '01111110': 0.00341796875, '01111111': 0.0234375, '10000000': 0.02734375, '10000001': 0.001953125, '10000010': 0.001953125, '10000011': 0.00390625, '10000100': 0.00048828125, '10000111': 0.00244140625, '10001100': 0.0009765625, '10001110': 0.00048828125, '10001111': 0.00341796875, '10010000': 0.001953125, '10011111': 0.0029296875, '10100000': 0.00146484375, '10110000': 0.0009765625, '10110001': 0.00048828125, '10111000': 0.00048828125, '10111001': 0.00048828125, '10111011': 0.00048828125, '10111100': 0.00048828125, '10111111': 0.00146484375, '11000000': 0.02734375, '11000001': 0.00390625, '11000010': 0.00048828125, '11000011': 0.00537109375, '11000100': 0.00048828125, '11000111': 0.00244140625, '11001000': 0.0009765625, '11001100': 0.0009765625, '11001110': 0.00048828125, '11001111': 0.0029296875, '11010000': 0.0009765625, '11011100': 0.00048828125, '11011111': 0.001953125, '11100000': 0.02880859375, '11100001': 0.0029296875, '11100011': 0.005859375, '11100100': 0.0009765625, '11100110': 0.00048828125, '11100111': 0.00341796875, '11101000': 0.00146484375, '11101100': 0.00048828125, '11101110': 0.0009765625, '11101111': 0.0048828125, '11110000': 0.0322265625, '11110001': 0.0029296875, '11110010': 0.001953125, '11110011': 0.0048828125, '11110100': 0.00048828125, '11110101': 0.00048828125, '11110111': 0.001953125, '11111000': 0.02685546875, '11111001': 0.00244140625, '11111010': 0.0009765625, '11111011': 0.00390625, '11111100': 0.0205078125, '11111101': 0.00341796875, '11111110': 0.0234375, '11111111': 0.13427734375}\n"
     ]
    }
   ],
   "source": [
    "# Create chunks of lenght 8 from X and from that calculate the probability of each chunk\n",
    "num_words = len(X) // 8\n",
    "\n",
    "chunks = X.reshape(num_words, 8)\n",
    "\n",
    "# Convert to a list of strings\n",
    "chunks = [''.join(map(str, chunk)) for chunk in chunks]\n",
    "\n",
    "# Use np.unique to count the number of occurences of each chunk\n",
    "unique_words, counts = np.unique(chunks, return_counts=True)\n",
    "\n",
    "# Calculate the probability of each chunk\n",
    "probs = counts / num_words\n",
    "\n",
    "# Convert probabilities to a dictionary\n",
    "probabilities_X = {''.join(map(str, word)): prob for word, prob in zip(unique_words, probs)}\n",
    "\n",
    "print(f\"The probability of each chunk is {probs}\")\n",
    "print(f\"The probability of each chunk is {probabilities_X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ad53ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the original binary string is 16384 bits\n",
      "The length of the encoded binary string is 9516 bits\n",
      "The difference in length is 6868 bits\n"
     ]
    }
   ],
   "source": [
    "# Binary string generated previously is called X\n",
    "# Constants\n",
    "k = 8\n",
    "\n",
    "# Generate Huffman codes\n",
    "huffman_codes = huffman_code(probabilities_X)\n",
    "\n",
    "# Function to encode a list of symbols\n",
    "def encode_Huffman(X, huffman_codes, k):\n",
    "    encoded_string = ''\n",
    "    for i in range(0, len(X)-k, k):\n",
    "        chunk = X[i:i+k]\n",
    "\n",
    "        # If the last chunk is smaller than k, pad it with 0s\n",
    "        #if len(chunk) == k:\n",
    "            # Encode the chunk using the Huffman codes\n",
    "        encoded_string += huffman_codes[''.join(map(str, chunk))]\n",
    "    return encoded_string\n",
    "\n",
    "# Encode the binary string\n",
    "encoded_string = encode_Huffman(X, huffman_codes, k=8)\n",
    "len_encoded_string = len(encoded_string)\n",
    "\n",
    "# Compare lengths (original and encoded)\n",
    "print(f\"The length of the original binary string is {len(X)} bits\")\n",
    "print(f\"The length of the encoded binary string is {len_encoded_string} bits\")\n",
    "print(f\"The difference in length is {len(X) - len_encoded_string} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad357d",
   "metadata": {},
   "source": [
    "When compressing the generated binary string using the Huffman code with tuples of 8 symbols, the message is 6868 bits smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4f323da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to estimate tuple frequencies from the data\n",
    "def estimate_tuple_frequencies(X, k):\n",
    "    # Initialize the dictionary with all possible tuples of length k\n",
    "    tuples = list(itertools.product([0, 1], repeat=k))\n",
    "    tuple_frequencies = {tuple_: 0 for tuple_ in tuples}\n",
    "\n",
    "    # Count the number of occurrences of each tuple in the data\n",
    "    for i in range(len(X) - k + 1):\n",
    "        tuple_ = tuple(X[i:i+k])\n",
    "        tuple_frequencies[tuple_] += 1\n",
    "    return tuple_frequencies\n",
    "\n",
    "# Estimate tuple frequencies for k = 8\n",
    "tuple_frequencies = estimate_tuple_frequencies(X, k=8)\n",
    "\n",
    "# Plot the tuple frequencies\n",
    "plt.figure(figsize=(13, 7))\n",
    "plt.bar(range(len(tuple_frequencies)), tuple_frequencies.values())\n",
    "plt.xlabel('Tuple')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Tuple Frequencies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c892817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take the unique words and plot their frequencies\n",
    "unique_words, counts = np.unique(chunks, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(24, 10))\n",
    "\n",
    "# Use the unique words as x-tick labels\n",
    "plt.bar(range(len(unique_words)), counts)\n",
    "plt.xticks(range(len(unique_words)), unique_words, rotation=90)  # Rotate labels for better readability\n",
    "\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Frequencies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add86bf",
   "metadata": {},
   "source": [
    "The graph \"Tuple Frequencies\" represents the frquencies of each 8-bit tuple in the binary string X (previously generated). Each bar in the graph corresponds to one of the possible 256 unique tuples that can be formed with 8 bits. More specifically, the graph \"Word Frequencies\" shows the frequency of each unique word that was sampled from X. For both cases, we can observe that the frequencies of the tuples are not uniform, meaning some sequences are more likely than others. The tuples with the tallest bars are the most frequent and will therefore we assigned a shorter code to achieve compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3090326",
   "metadata": {},
   "source": [
    "The compression rate can be calculated without performing the actual encoding by calculating the expected length of the Huffman encoded data based on the frequencies of the tuples:\n",
    "$$\\text{Compression rate} = \\frac{\\text{expected length}}{\\text{original length}}$$\n",
    "The entropy is the expected length of each compressed word, so to find the compression rate without doing any encoding we can sum all entropies obtained from all probabilities of the sample and divide such value by the length of each word, which in this case is 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bff8bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the binary string is 4.632888076487937 bits\n",
      "The compression rate is 0.5791110095609922\n"
     ]
    }
   ],
   "source": [
    "# Calculate the entropy of the binary string X\n",
    "entropy_X = -np.sum(probs * np.log2(probs))\n",
    "print(f\"The entropy of the binary string is {entropy_X} bits\")\n",
    "\n",
    "# Compression rate\n",
    "compression_rate = entropy_X / 8\n",
    "print(f\"The compression rate is {compression_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab6198",
   "metadata": {},
   "source": [
    "**(4) Compress the binary string using Run Length Encoding (RLE) with a maximal stretch of $2^k$. Namely, for $k=3$, the string 000001100011111111.... is encoded as (0,4), (1,1), (0,2), (1,7)..., which is then encoded as (0,100), (1,001), (0,010), (1,111), which is then encoded as 0100 1001 0010 1111 (each stretch of \"1\"s or \"0\"s is encoded using $1+k$ bits. We subtract one from the length of the stretch because there are no stretches of length 0). Stretches longer than $2^k$ are separated into a stretch of $2^k$ and the remainder. Experiment with values of $k$ between 2-8 and report the compression rate for each $k$. Which $k$ attains the best compression rate?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bdaa91",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099c8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For k=2 the compression rate is 0.90252685546875\n",
      " For k=3 the compression rate is 0.751953125\n",
      " For k=4 the compression rate is 0.69427490234375\n",
      " For k=5 the compression rate is 0.740478515625\n",
      " For k=6 the compression rate is 0.8502197265625\n",
      " For k=7 the compression rate is 0.97119140625\n",
      " For k=8 the compression rate is 1.09259033203125\n"
     ]
    }
   ],
   "source": [
    "# RLE algorithm\n",
    "def RLE_encode(binary_string, k):\n",
    "    encoded_string = ''\n",
    "    count = 0\n",
    "    prev_char = binary_string[0]\n",
    "\n",
    "    for char in binary_string:\n",
    "        if char == prev_char:\n",
    "            count += 1\n",
    "            # Check if we need to split the run into a chunk\n",
    "            if count == 2**k:\n",
    "                encoded_string += f'{prev_char}{bin(count-1)[2:].zfill(k)}'\n",
    "                count = 0\n",
    "        else:\n",
    "            # Encode the remaining part of the run if count is not zero\n",
    "            if count > 0:\n",
    "                encoded_string += f'{prev_char}{bin(count-1)[2:].zfill(k)}'\n",
    "            prev_char = char\n",
    "            count = 1\n",
    "\n",
    "    # Encode the last run if count is not zero\n",
    "    if count > 0:\n",
    "        encoded_string += f'{prev_char}{bin(count-1)[2:].zfill(k)}'\n",
    "\n",
    "    return encoded_string\n",
    "\n",
    "# Calculate the compression rate of the binary string X using RLE for k = 2, 3, 4, 5, 6, 7, 8\n",
    "for k in range(2, 9):\n",
    "    # Encode the binary string using RLE\n",
    "    encoded_string = RLE_encode(X, k)\n",
    "\n",
    "    # Calculate the compression rate\n",
    "    compression_rate = len(encoded_string) / (len(X))\n",
    "\n",
    "    # Print the compression rate for k in [2, 8]\n",
    "    print(f\" For k={k} the compression rate is {compression_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076f9b3",
   "metadata": {},
   "source": [
    "We can observe that as $k$ increases, the compression rate decreases and reaches its lowest point for $k=4$ to then increase again. Therefore, for this binary string, the optimal value of $k$ is $4$ since it minimizes the compression rate and thus maximizes compression efficiency. Indeed, the smaller the compression rate, the shorter the encoded string is in respect to the original string and therefore effective compression is achieved. We can observe that for $k=8$ the compression rate is bigger than 1, which suggests that the encoded string is longer than the original string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b430cb",
   "metadata": {},
   "source": [
    "**(5) Bonus: Can you think about a way to improve the proposed RLE?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d147d3b",
   "metadata": {},
   "source": [
    "#### <span style=\"color: LIGHTgreen;\">Answer</span>\n",
    "One option to improve the proposed RLE would be to implement a two-level RLE. This implies that the first level compresses the data (original string) and the second level compresses the output of the first level. This approach can yield better results in terms of the compression rate for cases in which there are nested repetitive patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbb95e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
